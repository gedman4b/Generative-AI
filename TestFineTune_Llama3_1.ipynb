{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a3ca914-219e-40a9-bec4-5c17a643124b",
   "metadata": {},
   "source": [
    "## Fine-tune a pretrained Meta-Llama-3.1-8B model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2551cc-fa13-47a0-b03f-084e925204e8",
   "metadata": {},
   "source": [
    "#### There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch using ðŸ¤— Transformers Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37fad918-3f6b-4b96-85ad-b353148a1fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore insignificant warnings (ex: deprecation warnings)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1881723d-3bc7-40cf-80ec-9c33e1c54684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"mi-rei/ClinicalTrial-gov-LLaMA\", split=\"train\")\n",
    "ds = ds.select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "db4e7999-2451-41a8-9085-300abf5bba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.train_test_split(test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "25c92216-06e8-451e-a605-c727eed77ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST]<<SYS>>You are given the full description of a clinical trial. Please summarize it.<</SYS>> - At the first visit the following information will be collected about the participant: original diagnosis, the date and type of transplant, transplant conditioning regimen, cGVHD prophylaxis regimen, the time when oral cGVHD was first noticed, specific treatments for oral cGVHD, and any current medications. - At each visit, and before the participant begins phototherapy treatment, they will answer a series of questions asking about how their mouth feels and what they are able to eat. A clinical examination of the mouth will be performed and recorded and photographs will be taken of the inside of the mouth. - Participants will then receive phototherapy treatment. This will take approximately three minutes and will involve opening the mouth and closing the eyes. Following phototherapy, the participant will be asked several questions on how they tolerated the treatment. Phototherapy treatments will be done two or three times per week for a total of 24 treatments. After each treatment, if the participant has not experienced any discomfort, the phototherapy dose will be increased following a specific protocol. - After 24 treatments the participant will have the option to continue phototherapy treatments. [/INST] The purpose of this trial is to find out how effective Narrow Band-Ultraviolet Light B (NB-UVB) phototherapy is in treating oral cGVHD. NB-UVB Phototherapy involves exposing the inside of the mouth to light of a particular spectrum (a specific wavelength of light, 311nm) of the ultraviolet band, called NB-UVB. It is known that narrow band ultraviolet light therapy can improve symptoms in patients with skin chronic GVHD. </s>'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']['text'][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aa5ece-f0cc-4533-a848-bbe108f226a6",
   "metadata": {},
   "source": [
    "### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a532e83b-c496-482a-bcd1-005c8e1cfa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fb6fc556-1199-4195-a17a-d5e2480a578e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6e15d18c4f42859503b8187afc1f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50093ce5ed9b4995ad20d1793648c172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '</s>'})\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True)\n",
    "#print(\"Writing datasets to disk...\")\n",
    "#tokenized_datasets.save_to_disk('tk_datasets/llama3_1Finetune_clinical_trial_datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd291a5-3129-483c-90e2-20c5588a5fd5",
   "metadata": {},
   "source": [
    "### create a smaller subset of the full dataset to fine-tune on to reduce the time it takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f3ca1543-8f6d-49af-93e4-e22ea31c5998",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1600))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(400))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ab8e1-9d98-484f-9977-cdeabf8a942e",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2293685a-da96-4e9e-ab7c-eb1041184f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c118d243-f66e-4297-9d18-4321f84ab730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0271b3-4cf7-4ccc-b180-8ed7434fe141",
   "metadata": {},
   "source": [
    "### Monitor Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8e24bba8-efe2-4777-8cd8-36b059876a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", \n",
    "                                  eval_strategy=\"epoch\",\n",
    "                                  overwrite_output_dir=True,\n",
    "                                  save_strategy = \"epoch\",\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62624175-2a7f-479b-97fc-4a176858cb73",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b35826a0-a003-437f-aafa-0c98b99bfa3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff76009e13a4ba89592ccbac111f0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mi-rei/Cthalpaca-llama2-7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480a23f-179c-430d-a4be-bab00a171350",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1ef60119-5165-4624-9f44-e4103ad4934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f71b6-7a0f-488c-951e-97b9d00d8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00381d-aae9-4336-bbab-fa5910dba344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
